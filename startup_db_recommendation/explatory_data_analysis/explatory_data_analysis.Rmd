---
title: "Exploratory Data Analysis"
author: "K Iwasaki"
date: "September 15, 2017"
output:
  pdf_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd('C:/Users/K/Desktop/Project/JOB_HUNTING_MADE_EASY/startup_db_recommendation/explatory_data_analysis')
data = read.csv("article_after_processing10.csv")
library(plyr)
library(dplyr)
library(ggplot2)
library(ggmap)
library(maps)
library(mapdata)

```
# Overview

Data exploration serves some important objectives. For this exploration, we focus on understanding the data-set in order to decide

1. What algorithm we select based the dataset
2. What variables we use for algorithm training and prediction
3. What transformation are required for each variables based on the choice of algorithm

Note that since we have spend significant amount of time for data cleaning and feature creation in the preprocessing phase, this data exploration doesn't cover these items.

# Set-up

Before diving into detailed analysis, it is good to start with a high level picture. In this section, we look at summary statistics to see distribution of each variable, check a variable category (such as category/continuous) for variables, and validate NA values in each column. 

A few things to notice at this point:

- Most of the columns are categorical variables except money_raised, year founded, latitude, longitude and etc.
- Industry labels are a dummary variable in which 1 is assinged for a company if it belongs a industry and 0 otherwise.
- There are NA values in some columns. We have actually noticed this in the data preprocessing stage.
- Target variable for the algorithm is CompanyName because what we want to predict(recommend) is which startup matches user's interests. This is a multiclass classification problem.


```{r, fig.height = 4, fig.show='hold', fig.align='center'}
drops = c("title", "link", "excerpt", "Company", "money_raised", "linkedin_link",
          "Company_at_Linkedin", "Specialties", "Website", "Location",
          "zip_code","State",
          "Description", "Also.viewed", "Industry")
data = data[, !(names(data) %in% drops) ]

# nrow(data)
# ncol(data)

summary(data[, 1:13 ])

# check columns 1:13. Columns 13: have same format.
str(data[, 1:13 ])

# show columns with na
na = lapply(data, function(x) sum(ifelse(is.na(x) | x == "" | x == "not found", TRUE, FALSE)))
na[na > 0]

```

# Univariate Analysis

Investigate distribution of key variables that we are interested in using for recommendation engines. If a variable is extremely skewed, we might need to consider transformation. In this analysis, we focus on 1) year founded, 2) funding round, 3) money raised, 4) company size, 4) country and 5) headquarter location.

## - Year Founded
Unexpectedly, there are some companies founded before 1990. Given this recommendation engine focuses on "startups", we migth need to exlude the outliers who founded before 1990. without the outliers, the distribution is close to normal distribution.

```{r, fig.height = 3, fig.show='hold', fig.align='center'}
hist(data$Founded, main = "Year Founded", breaks = 50)

# check who are the outliers
outliers = data[data$Founded <= 1990,c("CompanyName","CompanySize", "Founded", "funding_round", "money_raised_float")]
outliers

# store index of outliers
outliers.index = as.numeric(rownames(outliers))

# plot without the outliers
hist(data$Founded[-outliers.index], main = "Year Founded without outliers", breaks = 50)

```

## - Industry
We assigned multiple industry labels for each company in the preprocessing phase. This is because cross-industry nature of startups. For example, Netflix is Internet company and at the same time entertainment and media company. The multi-labeling should help our choice of algoritms to incorporate user inputs better by recognizing multiple industry selection as well.

Back to the dataset, top 10 industries are typical industries for startups. There is no surprise.

```{r, fig.height = 3.5, fig.width = 6 , fig.show='hold', fig.align='center'}

# store column sum in a list
counts = data[, 32:46] %>%
  summarise_each(funs(sum))

# transpose the dataframe for barplot
counts.T = data.frame(total = t(counts), industries = rownames(t(counts) ))

# plot in a descending order
counts.T[1:10,] %>%
  ggplot(.,aes(x = reorder(industries, -total), y = total )) +
  geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Top 10 Industries") +
  theme(plot.title = element_text(hjust = 0.5, size=14)) +
  theme(axis.title.x=element_blank()) +
  ylab("Number of Companies")

```


## - Funding round

As intended, most companies are in Series B and Series C. Need to merge "series C" and "Series C".

```{r, fig.height = 3, fig.show='hold', fig.align='center'}
# counts = table(data$funding_round)
# counts

data$funding_round[data$funding_round == "series C"] = "Series C"

# remove the level does not occur ("series C")
data$funding_round = factor(data$funding_round)

counts = table(data$funding_round)
counts

prop.table(table(data$funding_round))

barplot(counts, main = "Funding Round", las = 2)

```
## - Money raised

The distribution is skewed to the right as most of companies raised money under $100M. We observe some outliers: Magic Leap, Pivotal, GitHub, and Opendoor.com. Unlike the outliers in the year founded variable, we don't consider removing this data-set because they are still within a definition of startup.

```{r, fig.height = 3.5, fig.show='hold', fig.align='center'}
summary(data$money_raised_float)

hist(data$money_raised_float, breaks = 40, main = "Money Raised")

# check the outliers
data[data$money_raised_float > 200, c("CompanyName", "funding_round", 
                                      "CompanySize","money_raised_float")]

```

## - Number of Employees

The disribution is close to normal distribution with a peak at "51-200". But it has outliers as same to other variables: there are two companies with more than 10,000 employees. We don't normally call them startups with that size. I suspect this is because of M&A. These companies might have been purchased by the large corporation and their company size reflect their acquirers.

```{r, fig.height = 3, fig.show='hold', fig.align='center'}
# counts = table(data$CompanySize)
# counts

# str(data$CompanySize)

# clean up - factors
data$CompanySize = revalue(data$CompanySize, c("Nov-50"="11-50", "10-Jan"="1-10"))

# clean up - the level orders
data$CompanySize = factor(data$CompanySize, levels = c(
  "1-10", "11-50","51-200","201-500", "501-1000", "1001-5000","5001-10,000","10,001+"))  

counts = table(data$CompanySize)
counts

barplot(counts, main = "Company size", las=2)

# check the outliers
data[data$CompanySize == "10,001+", c("CompanyName", "Founded", "CompanySize","money_raised_float")]

```

## - Country

Since I collected startups from TechCrunch, the US-based news outlet, it turns out 77% startup in the dataset are based in the US. This might also be because the US produces the largest number of startups.

```{r}
counts = table(data$Country)
counts

prop.table(counts)

```

## - City

The location of startups comes with no suprise. The ranking tops San Francisco, then New York, Moutain View, San Mateo, and Boston.

```{r, fig.height = 4, fig.show='hold', fig.align='center'}
detach(package:plyr)
data %>%
  group_by(City) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  filter(City != "") %>%
  slice(1:10) %>%
  ggplot(., aes(x = reorder(City, -n), y = n)) +
  geom_bar(stat = "identity") +
  ggtitle("Top 10 Cities") +
  theme(plot.title = element_text(hjust = 0.5, size=14)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  theme(axis.title.x=element_blank()) +
  ylab("Number of Companies")
  
```


# Bivariate Analysis

Analyze the relationship between two variables. Usually it serves two purposes: 1) look at the association between independent variables (explanatory variable) and target variable (in our case, CompanyName) in order to select variables to include in a model to build 2) look at the associations among independent variables to remove highly correlated variables from the inputs for the model.  Since our problem is extreme multiclass classification problem where each example has different target variable (company name), we don't run the former analysis described above. Instead, we focus on the latter analysis. Specifically, we look at the relationships for the following variable combinations:

- Company size x Year Founded
- Company size x Money Raised
- Funding round x Money raised
- Location  x Money Raised
- Location  x Specific industries

## - Company_size x Year_founded

```{r, fig.height = 4,fig.width = 5, fig.show='hold', fig.align='center'}

# remove outliers (companies founded before 1990)
outliers = data[data$Founded <= 1990,]
outliers.index = as.numeric(rownames(outliers))

# plot
ggplot(data[-outliers.index,], aes(x= Founded, y = CompanySize)) +
  geom_jitter() +
  ggtitle("Year Founded x Company Size") +
  theme(plot.title = element_text(hjust = 0.5, size=14))

```

## - Company_size x Money_raised

```{r, fig.height = 4,fig.width = 5, fig.show='hold', fig.align='center'}

# remove outliers
outliers = data[data$money_raised_float > 200, ]
outliers.index = as.numeric(rownames(outliers))

# plot
ggplot(data[-outliers.index,], aes(x= money_raised_float, y = CompanySize)) +
  geom_jitter() +
  ggtitle("Money Raised x Company Size") +
  theme(plot.title = element_text(hjust = 0.5, size=14))

```
```{r, fig.height = 3, fig.show='hold', fig.align='center'}
money = data %>%
  group_by(CompanySize) %>%
  summarize(mean = mean(money_raised_float), sd = sd(money_raised_float))

counts = money$mean
names(counts) = money$CompanySize
barplot(counts, las = 2, main = "Average Money Raised by Company Size")

```

## - Funding_round x Money_raised

```{r, fig.height = 4,fig.width = 5, fig.show='hold', fig.align='center'}

# remove outliers
outliers = data[data$money_raised_float > 200, ]
outliers.index = as.numeric(rownames(outliers))

# plot
ggplot(data[-outliers.index,], aes(x= money_raised_float, y = funding_round)) +
  geom_jitter() +
  ggtitle("Funding Round x Money Raised") +
  theme(plot.title = element_text(hjust = 0.5, size=14))

```

The table below shows mean and standard deviaiton of money raised for companies in each funding round. It makes sense that the mean increases as funding round progresses. Series E has lower mean than Series D. This might be because Series E is more of extension of Series D to sustain funding and not a funding round to drive a company to next level. Also note that stand devidations are quite larger for each round.

```{r, fig.height = 6, fig.show='hold', fig.align='center'}

par(mfrow=c(2,1))

money = data %>%
  group_by(funding_round) %>%
  summarize(sum = sum(money_raised_float), mean = mean(money_raised_float), sd = sd(money_raised_float))
money

counts = money$sum
names(counts) = money$funding_round
barplot(counts, las = 2, main = "Total Money Raised by Funding Round")

counts = money$mean
names(counts) = money$funding_round
barplot(counts, las = 2, main = "Average Money Raised by Funding Round")

```

## - Location x Money_Raised

```{r, message=F, warning=F}

# Set the map
all_states <- map_data("state")

# plot
ggplot() + geom_polygon(data = all_states, aes(x=long, y = lat, group = group), 
                        colour="white", fill="grey10") + 
  coord_fixed(1.3) +
  geom_jitter(data = data, mapping = aes(
    x = longitude, y = latitude, color = "Orangered" ,size = money_raised_float),
             alpha = 0.4, width = 0.7, height = 0.7) +
  scale_size(range = c(2, 12)) +
  ggtitle("Company Location with Money Raised") +
  theme(plot.title = element_text(hjust = 0.5, size=18)) +
  labs(x=NULL, y=NULL) +
  theme(panel.border = element_blank())

```

## - Location x Money_Raised for FinTech Startups

```{r}
# Set the map
all_states <- map_data("state")

# plot
ggplot() + geom_polygon(data = all_states, aes(x=long, y = lat, group = group), 
                        colour="white", fill="grey10") + 
  coord_fixed(1.3) +
  geom_jitter(data = data[data$Financial.Services == 1,], mapping = aes(
    x = longitude, y = latitude, color = "orangered" ,size = money_raised_float),
             alpha = 0.4, width = 0.7, height = 0.7) +
  scale_size(range = c(2, 12)) +
  ggtitle("Fintech Startups with Money Raised") +
  theme(plot.title = element_text(hjust = 0.5, size=18)) +
  labs(x=NULL, y=NULL) +
  theme(panel.border = element_blank())
```

## - Location x Money_Raised for Entertainment Startups

```{r}
# Set the map
all_states <- map_data("state")

# plot
ggplot() + geom_polygon(data = all_states, aes(x=long, y = lat, group = group), 
                        colour="white", fill="grey10") + 
  coord_fixed(1.3) +
  geom_jitter(data = data[data$Entertainment == 1,], mapping = aes(
    x = longitude, y = latitude, color = "Orangered" ,size = money_raised_float),
             alpha = 0.4, width = 0.7, height = 0.7) +
  scale_size(range = c(2, 12)) +
  ggtitle("Entertainment Startups with Money Raised") +
  theme(plot.title = element_text(hjust = 0.5, size=18)) +
  labs(x=NULL, y=NULL) +
  theme(panel.border = element_blank())
```

# Conclusion

Follow up the objectives set at the beginning of this data exploration.

**- Algorithm Selection**

Choose K-Nearest Neighbor for this problem because this is a multiclass classification problem where each example of training data has different target label (CompanyName). Also, the sample size is quite small for now. This is good for KNN.

**- Feature Selection**

KNN requires dimension reduction because it doesn't handle multidimensional space well. Thus, we need to select some variables that really matters for the model to make predictions. I selected following variables based on bi-variate analysis: money_raised, company_size, location, year_founded, and industry. 

**- Feature Transformation**

KNN assumes each feature to be a numerical variable and scaled properly because KNN is a distance-based algorithm which calculate distance between each test sample and each training test example. It's also important to note that KNN is sensitive to outliers because again it's distance-based algorithm. Given these in mind, we need to make following feature transformations for each variables we selected as the model inputs.

- Money_raised - log transformation for outlier handling and min-max transformation
- Company size - Binning and min-max transformation
- Location - latitude and longitude of cities and min-max transformation for them
- Year Founded - removal of outliers and min-max transformation
- Industry - dummy variable transformation (already done)








